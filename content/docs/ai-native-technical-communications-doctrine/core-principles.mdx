---
title: "Core Principles"
description: "Foundational principles for AI-native documentation: modularity, fact-basis, prompt-ready structure, and embedded system guidance."
icon: Scale
intended-audience: ["Engineers", "Product Managers", "Technical Writers", "LLM Prompt Engineers"]
version: "1.0"
ai-context:
  prompt-grounding: |-
    - Modularity: break content into small, standalone modules aligned to likely questions.
    - Clear hierarchy: consistent headings and scopes; one concept per section.
    - Fact basis: definitive statements with defaults, limits, versions, and side effects.
    - Prompt-ready style: Q&A headings, second-person instructions, and self-explanatory examples.
    - Terminology consistency: one term per concept across the corpus.
    - Embedded guidance: frontmatter, notes, and system cues for version, audience, and safety.
    - Visual parity: pair images with textual descriptions; prefer Mermaid for ingestible diagrams.
---


What does it mean to write documentation *for AI first*? At a high level, it means applying a set of principles that ensure the content is *machine-navigable, unambiguous, and readily usable in an AI conversation*. Here are the core principles of AI-native documentation, which form the foundation of this doctrine:

* **Modularity:** Break documentation into small, self-contained modules or sections, each focused on a single concept, feature, or task. Each module should ideally answer one primary question or serve one specific use-case. This modularity aligns with how LLMs retrieve information: they fetch the few chunks most relevant to a query. If your product documentation is one giant monolith, the AI may retrieve a large irrelevant chunk along with the relevant info, leading to diluted or confusing answers. Modular docs ensure that when a question is asked, the AI can pull just the right piece. For example, in a Redis guide, instead of a single page covering all commands, each command (GET, SET, EXPIRE, etc.) would be a module with its description and examples. A question about the “EXPIRE” command would retrieve only that module, not an entire commands reference. Think of modules as atomic facts or instructions – small building blocks that can be assembled to answer complex queries.
* **Clear Structure and Hierarchy:** Use a consistent, logical heading hierarchy and formatting conventions so the AI can understand relationships between topics. Each page or section should have a clear title, and sub-sections that descend one level at a time (avoiding skipping heading levels). A well-structured document is like a well-formed outline – it signals to the AI what each part is about. Consistent hierarchy also prevents unrelated info from being lumped together. If you have an H2 for “Installation” and another H2 for “Configuration”, don’t suddenly tuck a new topic under an H4 somewhere without intermediate H3s – it might get lost. Structure is not just for humans; it’s a map for AI on how content is segmented. As Kapa.ai’s team observed, *“properly structured documentation can significantly improve LLMs’ ability to understand and respond to user queries”*.
* **Fact-Based, Source-of-Truth Writing:** Emphasize factual, definitive statements and avoid fluff or ambiguous commentary. LLMs are superb at using facts provided to construct answers, but they can also be misled by speculation or marketing language in docs. Write documentation content as if you’re populating a knowledge base – each statement should be correct and verifiable. Wherever possible, quantify and specify. For instance, instead of “The application uses very little memory,” say “The application uses approximately 50 MB of memory when idle.” The AI will then confidently include those specifics in an answer about memory usage. Fact-based writing also means including default values, limits, versions, etc., directly in the text. Essentially, we want to minimize scenarios where the AI has to “guess” or use its general training to fill gaps – the docs should supply the facts. Ensuring accuracy in docs is the first guardrail against AI hallucination. If the docs cover most questions with factual answers, the AI has less room or need to make something up.
* **Prompt-Ready Style:** Write content in a way that it can be directly used in an AI’s response with minimal alteration. This involves a few tactics:

  * Use the user’s perspective in descriptions (“To do X, **you** should...”) judiciously, because the AI often outputs answers addressing the user as “you.” If the documentation is written from a third-person perspective (“The user should do X”), the AI might awkwardly convert that. Consider writing steps and explanations in a way that an AI could lift them as-is into a helpful reply. Many docs already do this for clarity (second-person, imperative mood), which doubles as AI-ready phrasing.
  * Anticipate likely questions and phrasing, and mirror those in the documentation. For example, a heading like “How to reset the password for a user” is both human-friendly and likely to match a user’s question. Including Q\&A formatted sections or an FAQ is a great way to have prompt-ready content. If an FAQ entry literally poses the question a user might ask, the AI can spot that and directly use the provided answer.
  * Ensure that examples and references are self-explanatory. A code block should usually be accompanied by a one-line description of what it is (“Example of a POST request to the login API”). That way, if the AI includes the example in an answer, it has the description to introduce it. Similarly, avoid references to external context (“see the figure above”) – the AI won’t include the figure, so that text becomes meaningless in the answer. Always assume the content might be excerpted on its own. Each module should contain enough context to be understood independently.
* **Consistency and Terminology:** Establish consistent terms, naming, and style across the docs. This might not sound “AI-specific”, but it is crucial for AI. LLMs use semantic patterns to retrieve text; if you refer to the same concept by different names in different places (e.g., “access token” vs “auth token” vs “token”), the AI might not link those together and could miss relevant info. Pick one term for one concept and use it uniformly. Consistency also applies to formatting – e.g., always format code as code (with backticks or code blocks) when referring to it. If sometimes you write a command in plain text and elsewhere as `code`, the AI might tokenize them differently. A consistent style acts like a schema that the AI can learn. It also reduces the chance of retrieval errors where, say, the AI doesn’t realize an acronym used in one part of the docs is the same as a spelled-out term in another. If acronyms are necessary, define them clearly and perhaps list them in a glossary section (which the AI can consult). Think of it this way: treat the documentation like code – consistency and lack of ambiguity help avoid “bugs” in the AI’s understanding.
* **Embedded Guidance and Metadata:** One novel principle of AI-native docs is including guidance *inside* the documentation that helps the AI use it correctly. This can be done through system prompts, comments, or metadata fields that are part of your docs pipeline. For example, you might include an internal note in a doc page: “> NOTE (for AI): This section is version-specific. Ensure the user has specified the version.” A human reader might ignore this or not see it if it’s hidden metadata, but an AI system could be configured to respect such notes, adjusting its answer or asking the user for their version. Another example is embedding a “role” or audience in the frontmatter metadata of a document (e.g., intended-audience: expert or beginner). An AI could use that to tailor the detail level of its answer based on who is asking. System prompts (the hidden instructions given to an AI before it answers) can also be informed by documentation content. Some implementations include a “instructions.md” file in the docs that contains guidelines like “When answering questions about the API, if the request is about authentication, remind the user about security best practices.” This can be ingested and always prepended as part of the AI’s context. While not all AI systems will automatically use such embedded guidance, designing your docs with this in mind is forward-looking. It ensures that as your AI assistants become more sophisticated, they have the metadata and cues needed to enforce policies (security, confidentiality) and context (e.g., which product the user is asking about) automatically. Practically, this principle encourages using frontmatter YAML in each doc with fields for things like version, product, module, sensitivity, etc., and populating them diligently. These metadata fields don’t matter to a human reader, but they are gold for an AI pipeline. They can be used to filter search results (e.g., retrieve only docs for “version: 3.x” if the user is on version 3), to format answers (e.g., mention the version), or to apply the correct tone (user documentation vs internal technical note). In essence, write your docs as if you’re teaching an AI assistant how to do the job of a documentation expert – because that’s what you’re doing.

To illustrate these principles, let’s imagine a simple example from an open-source project like Kubernetes. In a human-first world, you might have a single page “Networking” that explains various networking concepts (Services, Ingress, DNS, etc.) in a long form. In an AI-first approach, you would modularize this into distinct pages or sections: one for Services, one for Ingress, one for DNS configuration, etc. Each of those pages would start with a clear definition (“Ingress is a Kubernetes object that allows external access to services, typically via HTTP/HTTPS...”), list key facts (maybe bullet points of what it does and doesn’t do), and perhaps include an example YAML config with an explanation. The frontmatter might tag each page with `area: networking` and `version: ["1.26","1.27"]` if relevant. If a user asks the AI “How do I expose a service externally in Kubernetes?”, the assistant might retrieve the “Ingress” module and the “Service” module. Because those modules are focused and labeled, it will find the exact instructions and example needed, and because we wrote them in a straightforward, you-oriented way, it can almost copy-paste the steps into the answer. If something is version-specific (maybe Ingress behaves differently in older versions), our docs’ metadata or embedded notes would alert the AI to clarify the Kubernetes version, rather than giving a generic answer.

It’s worth noting that these core principles benefit *human* readers as well. Clarity, modularity, and factual accuracy are hallmarks of good documentation in general. The difference is that in an AI-first doctrine, we are uncompromising about these principles because the AI won’t “forgive” unclear writing the way a human might. A human reader might puzzle through a muddy sentence or infer meaning from context; an AI could misinterpret it entirely or ignore it. By adhering to these principles, we create documentation that is dual-purpose: it serves the AI in delivering quick answers, and it remains perfectly accessible to any human who reads it directly. In fact, many of these ideas align with what some call “Generative Engine Optimization (GEO)” – making content readable and useful to LLMs.

In summary, the core doctrine for AI-native writing is: *write less like you’re writing a book, and more like you’re building a knowledge database*. Every section is an API endpoint the AI can call; every sentence is a potential fact it might quote. In the next section, we build on these principles and talk about concrete techniques for architecting documentation systems and content that fully embrace the AI as a first-class consumer.
